<!doctype html>
<html lang="en">
  <head><script src="/class/cs329h/slides/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=class/cs329h/slides/livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<title>Chapter 2: Choice Models</title>


<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><link rel="stylesheet" href="/class/cs329h/slides/reveal-js/dist/reset.css">
<link rel="stylesheet" href="/class/cs329h/slides/reveal-js/dist/reveal.css"><link rel="stylesheet" href="/class/cs329h/slides/css/serif.css" id="theme"><link rel="stylesheet" href="/class/cs329h/slides/highlight-js/default.min.css">
  </head>
  <body>
    
    <style>
      #logo {
        position: absolute;
        top: 1%;
        left: 1%;
        width: 15%;
      }
    </style>
    <img id="logo" src="../images/sail-logo.jpg" alt="">
    
    <div class="reveal">
      <div class="slides">
  

    
<section data-noprocess data-shortcode-slide
      data-auto-animate="">
  
<h2 id="human-preference-models">Human Preference Models:</h2>
<h2 id="choice-models-continue">Choice models (Continue)</h2>
<p>Sanmi Koyejo, Sang Truong, Kenan Hasanaliyev</p>
<p><a href='http://localhost:1313/class/cs329h/slides/#/1'> All Sections </a></p>
</section>

<section data-noprocess data-shortcode-slide
      data-auto-animate="">
  
<h3 id="the-ideal-point-model">The ideal point model</h3>
<ul>
<li>An embedding approach, assumes user item preference depends on distance
<ul>
<li>Let $x_n$ denote a latent vector representing an individual $n$</li>
<li>Let $v_i$ denote a latent vector representing choice (or item) $i$ $U_{ni} = dist(x_n, v_i) + \epsilon_{ni}$</li>
<li>Model is equivalent to choosing the “closest” item</li>
</ul>
</li>
</ul>
<p>$$y_{ni} = 
\begin{cases} 
1, & \text{if } U_{ni} > U_{nj} \ \forall j \neq i \\
0, & \text{otherwise} 
\end{cases}$$</p>
</section>

<section data-noprocess data-shortcode-slide
      data-auto-animate="">
  
<h3 id="ideal-point-model-the-why">Ideal point model: the why</h3>
<ul>
<li>Pros: Can sometimes learn preferences faster than attribute-based preference models by exploiting geometry (see refs)</li>
<li>Cons:
<ul>
<li>Embedding assumption may be strong (can make more flexible via distance function choice)</li>
<li>However, have to select a distance function (usually use Euclidian distance in the embedding)</li>
</ul>
</li>
</ul>
<p style="text-align: left; font-size: 16px;">Jamieson, Kevin G., and Robert Nowak. "Active ranking using pairwise comparisons."
<br>
Tatli, Gokcan, Rob Nowak, and Ramya Korlakai Vinayak. "Learning Preference Distributions From Distance Measurements."</p>
</section>

<section data-noprocess data-shortcode-slide
      data-auto-animate="">
  
<h3 id="choice-models-in-rl-and-rlhf">Choice models in RL (and RLHF)</h3>
<p><img src="figures/choice_of_rl.png" alt="Choice models in RL"></p>
</section>

<section data-noprocess data-shortcode-slide
      data-auto-animate="">
  
<h3 id="application-rl-and-language">Application: RL and Language</h3>
<h3 id="bradley-terry-model">(Bradley-Terry model)</h3>
<p><img src="figures/rlhf.png" alt="RLHF"></p>
<p style="text-align: center; font-size: 16px;">https://openai.com/research/learning-to-summarize-with-human-feedback</p>
</section>

<section data-noprocess data-shortcode-slide
      data-auto-animate="">
  
<h4 id="choice-models-in-ml-recommender-systems-bandits-direct-preference-optimization">Choice models in ML (recommender systems, bandits, Direct Preference Optimization)</h4>
<p><img src="figures/model_in_ml.png" alt="Model in ML"></p>
</section>

<section data-noprocess data-shortcode-slide
      data-auto-animate="">
  
<h4 id="choice-models-in-ml-recommender-systems-bandits-dpo">Choice models in ML (recommender systems, bandits, DPO)</h4>
<p><img src="figures/model_in_ML2.png" alt="Model in ML"></p>
</section>

<section data-noprocess data-shortcode-slide
      data-auto-animate="">
  
<h3 id="why-dpo">Why DPO?</h3>
<p><img src="figures/dpo_vs_ppo.png" alt="DPO vs PPO"></p>
<ul>
<li>RLHF pipeline is complex and unstable due to the reward model optimization.</li>
<li>DPO is more stable and can be used to optimize the reward model directly.</li>
</ul>
<p style="text-align: left; font-size: 16px;">Rafael Rafaelov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn, "Direct preference optimization: Your language model is secretly a reward model."</p>
</section>

<section data-noprocess data-shortcode-slide
      data-auto-animate="">
  
<h3 id="dpo-bradley-terry-model">DPO: Bradley-Terry model</h3>
<ul>
<li>Given prompt $x$ and completions $y_w$ and $y_l$ the choice model gives the preference</li>
</ul>
<p>$$p^*(y_w > y_l | x) = \frac{\exp(r^*(x, y_w))}{\exp(r^*(x, y_w)) + \exp(r^*(x, y_l))}$$</p>
<p>where $r^*(x, y)$ is some latent reward model that we do not have access to (i.e., the human preference)</p>
</section>

<section data-noprocess data-shortcode-slide
      data-auto-animate="">
  
<h3 id="dpo-bradley-terry-model-1">DPO: Bradley-Terry model</h3>
<p>Luckily, we can use parameterize the reward model with some neural networks with parameters $\phi$:</p>
<p>Let us start with the Reward Maximization Objective in RL:</p>
<p>$$\max_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(y|x)} [r_\phi(x, y) - \beta D_{KL}(\pi_\theta(y|x) \| \pi_{\text{ref}}(y|x))]$$</p>


<span class='fragment ' ><ul>
<li>Where $\pi_\theta(y|x)$ is the language model, and $\pi_{\text{ref}}(y|x)$ is the reference model (e.g., the language model before fine-tuning)</li>
</ul>
</span>


</section>

<section data-noprocess data-shortcode-slide
      data-auto-animate="">
  
<p>$$\max_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(y|x)} [r_\phi(x, y) - \beta D_{KL}(\pi_\theta(y|x) \| \pi_
{\text{ref}}(y|x))]$$</p>
<p>Recall the definition of KL divergence:</p>
<p>$$D_{KL}(p \| q) = \sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)} = \mathbb{E}_{x \sim \mathcal{X}} \left[ \log \frac{p(x)}{q(x)} \right]$$</p>
<p>Then we can rewrite the objective as:</p>
<p>$$\begin{aligned}
&\max_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(y|x)} \left[ r_\phi(x, y) - \beta \mathbb{E}_{y \sim \pi_\theta(y|x)} \left[\log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)} \right] \right]\\
&=\max_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi_\theta(y|x)} \left[ r_\phi(x, y) - \beta \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)} \right]
\end{aligned}$$</p>
</section>
<p>
<section data-noprocess data-shortcode-slide
      data-auto-animate="">
  
Then, we can continue to derive the objective as:</p>
<p>$$\begin{aligned}
    &\max_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi_\theta(y|x)} \left[ r_\phi(x, y) - \beta \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)} \right] \\
    &\propto \min_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi_\theta(y|x)} \left[ \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)} - \frac{1}{\beta} r_\phi(x, y) \right] \text{// reverse and divide } \beta\\
    &= \min_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi_\theta(y|x)} \left[ \log \frac{\pi_\theta(y|x)}{\frac{1}{Z(x)} \pi_{\text{ref}}(y|x) \exp\left(\frac{1}{\beta} r_\phi(x, y)\right)} - \log Z(x) \right]
\end{aligned}$$</p>
<p>$$\text{with} \quad Z(x) = \sum_{y} \pi_{\text{ref}}(y|x) \exp\left(\frac{1}{\beta} r_\phi(x, y)\right)$$</p>
</section>
<p>
<section data-noprocess data-shortcode-slide
      data-auto-animate="">
  
Because $Z(x)$ is a constant with respect to $\pi_\theta$, we can define:</p>
<p>$$\pi^*(y|x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y|x) \exp\left(\frac{1}{\beta} r_\phi(x, y)\right)$$</p>
<p>Then, we can rewrite the optimization problem as:</p>
<p>$$\min_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi_\theta(y|x)} \left[ \log \frac{\pi_\theta(y|x)}{\pi^*(y|x)} - \log Z(x) \right]$$</p>
<p>$$= \min_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi_\theta(y|x)} \left[ \mathbb{D}_{KL}(\pi_\theta(y|x) \| \pi^*(y|x)) - \log Z(x) \right]$$</p>
<p>Thus, the optimal solution (i.e., the optimal language model) is:</p>
<p>$$\pi_\theta(y|x) = \pi^*(y|x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y|x) \exp\left(\frac{1}{\beta} r_\phi(x, y)\right)$$</p>
</section>
<p>
<section data-noprocess data-shortcode-slide
      data-auto-animate="">
  
With some algebra, we can show that the optimal reward model is:</p>
<p>$$\begin{aligned}
\pi_\theta(y|x) &= \frac{1}{Z(x)} \pi_{\text{ref}}(y|x) \exp\left(\frac{1}{\beta} r_\phi(x, y)\right)\\
\log \pi_\theta(y|x) &= \log \pi_{\text{ref}}(y|x) + \frac{1}{\beta} r_\phi(x, y) - \log Z(x) \text{// perform  } \log(.)\\
r_\phi(x, y) &= \beta \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)} + \beta \log Z(x)\\
\end{aligned}$$</p>
</section>
<p>
<section data-noprocess data-shortcode-slide
      data-auto-animate="">
  
Recall the Bradley-Terry model with parameterized reward model:</p>
<p>$$p_\phi(y_w > y_l | x) = \frac{\exp(r_\phi(x, y_w))}{\exp(r_\phi(x, y_w)) + \exp(r_\phi(x, y_l))}$$</p>
<p>We also have the optimal reward model:</p>
<p>$$r_\phi(x, y) = \beta \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)} + \beta \log Z(x)$$</p>
<p>Thus, we can rewrite the choice model as:</p>
<p>$$\begin{aligned}
p_\phi(y_w \succ y_l | x) &= \frac{1}{1 + \exp\left( \beta \log \frac{\pi_\theta(y_l | x)}{\pi_{\text{ref}}(y_l | x)} - \beta \log \frac{\pi_\theta(y_w | x)}{\pi_{\text{ref}}(y_w | x)} \right)}\\
&= \sigma\left( \beta \log \frac{\pi_\theta(y_w | x)}{\pi_{\text{ref}}(y_w | x)} - \beta \log \frac{\pi_\theta(y_l | x)}{\pi_{\text{ref}}(y_l | x)} \right)
\end{aligned}$$</p>
</section>

<section data-noprocess data-shortcode-slide
      data-auto-animate="">
  
<h3 id="dpo-bradley-terry-model-2">DPO: Bradley-Terry model</h3>
<p>Recall our objective to maximize the reward model, we can rewrite the objective as maximizing the likelihood of the choice model:</p>
<p>$$\mathcal{L} (r_\theta, \mathcal{D}) = - \mathbb{E}_{(x, y_w, u_l) \sim \mathcal{D}} \left[ \log p_\phi(y_w \succ y_l | x) \right]$$</p>
<p>Finally, we can rewrite the objective as:</p>
<p>$$\begin{aligned}
\mathcal{L}_{DPO}(\pi_\theta; \pi_{\text{ref}}) &= - \mathbb{E}_{(x, y_w, u_l) \sim \mathcal{D}} \left[ \log p_\phi(y_w \succ y_l | x) \right]\\
&= -\mathbb{E}_{(x, y_w, y_l) \sim D} \left[ \log \sigma\left( \beta \log \frac{\pi_\theta(y_w | x)}{\pi_{\text{ref}}(y_w | x)} - \beta \log \frac{\pi_\theta(y_l | x)}{\pi_{\text{ref}}(y_l | x)} \right) \right]
\end{aligned}$$</p>
<p style="text-align: left; font-size: 16px;">Rafael Rafaelov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn, "Direct preference optimization: Your language model is secretly a reward model."</p>
</section>

<section data-noprocess data-shortcode-slide
      data-auto-animate="">
  
<p><img src="figures/rlhf_comparison.png" alt="RLHF Comparison"></p>
</section>

<section data-noprocess data-shortcode-slide
      data-auto-animate="">
  
<h3 id="should-your-ml-application-use-an-explicit-utilityreward-model">Should your ML application use an explicit utility/reward model?</h3>
<ul>
<li>Pro:
<ul>
<li>Reward models can be re-used (in principle)</li>
<li>Reward model can be examined to infer properties of human(s), and measure the quality of the preference model(s)</li>
<li>Reward model(s) add useful inductive biases to the training pipeline</li>
</ul>
</li>
<li>Cons:
<ul>
<li>The extra step of reward modeling can introduce (unnecessary?) errors</li>
<li>Reward model optimization can be unstable (e.g., in RLHF, as argued by DPO)</li>
</ul>
</li>
</ul>
</section>

<section data-noprocess data-shortcode-slide
      data-auto-animate="">
  
<h3 id="some-criticisms-of-choice-modeling-more-broadly">Some criticisms of choice modeling more broadly</h3>
<ul>
<li>Real-world choices often appear to be highly situational or context-dependent e.g., way choice is posed, emotional states, other factors not well modeled.
<ul>
<li>Arguably what is exploited by marketing. Related to framing effects (more later).</li>
<li>A partial rebuttal: In principle, can always add more context to the model.</li>
</ul>
</li>
<li>Many choices are intuitive rather than rational, so utility optimization models do not apply
<ul>
<li>Please have limited attention and cognitive capability, especially for less salient choices</li>
<li>Default choices are powerful, e.g., in 401K, or opt-in organ donors</li>
</ul>
</li>
</ul>
</section>

<section data-noprocess data-shortcode-slide
      data-auto-animate="">
  
<h2 id="q--a">Q &amp; A</h2>
<ul>
<li>What are some key assumptions in (discrete) choice models?
<ul>
<li>Rationality (existence of a utility function that determines choices)</li>
<li>Parametric model for utility and choice noise</li>
<li>Finite set of choices, and explicit alternatives</li>
</ul>
</li>
<li>How does one apply discrete choice models to ML/RL applications with changing context (input)
<ul>
<li>Model utility via generic models (e.g., deep neural networks)</li>
</ul>
</li>
<li>What are some criticisms of discrete choice models?
<ul>
<li>Humans display context-dependent choices</li>
<li>Humans often make intuitive (or irrational) choices</li>
</ul>
</li>
</ul>
</section>

<section data-noprocess data-shortcode-slide
      data-auto-animate="">
  
<h2 id="what-is-not-covered">What is not covered</h2>
<ul>
<li><strong>Details of estimation, analysis</strong>
<ul>
<li>Maximum likelihood is generally equivalent to standard classification/ranking</li>
<li>Existing analysis (though often interesting) is mostly for linear (or simpler) utilities</li>
<li>Many of the interesting theoretical questions are for active querying settings</li>
</ul>
</li>
<li><strong>Beyond discrete choice models</strong>
<ul>
<li>With equivalent alternatives ($U_1 &gt; U_2, U_1 \approx U_3$)</li>
<li>Continuous &ldquo;choices&rdquo; e.g., pricing, demand/supply</li>
<li>Dynamic discrete choice (for time varying choices) $\approx$ RL</li>
</ul>
</li>
<li><strong>Experimental design for &ldquo;stated preferences&rdquo;</strong>
<ul>
<li>How to design a survey to measure alternatives, conjoint analysis</li>
</ul>
</li>
<li><strong>Active querying</strong> (future discussion)</li>
</ul>
</section>

<section data-noprocess data-shortcode-slide
      data-auto-animate="">
  
<h2 id="summary">Summary</h2>
<ul>
<li><strong>Today:</strong> Overview of discrete choice models
<ul>
<li>Basics of discrete choice and rationality assumptions</li>
<li>Benefits and criticisms of discrete choice</li>
<li>Some special cases and applications of discrete choice models to ML</li>
</ul>
</li>
<li><strong>Next Lecture:</strong> Student discussion on Human Decision Making and Choice Models</li>
</ul>
</section>

<section data-noprocess data-shortcode-slide
      data-auto-animate="">
  
<h2 id="references">References</h2>
<small>
<ul>
  <li>Train, K. (1986). <em>Qualitative Choice Analysis: Theory, Econometrics, and an Application to Automobile Demand.</em> MIT Press. ISBN 9780262200554. Chapter 8.</li>
  <li>McFadden, D.; Train, K. (2000). "Mixed MNL Models for Discrete Response" (PDF). <em>Journal of Applied Econometrics.</em> 15 (5): 447–470.</li>
  <li>Luce, R. D. (1959). <em>Individual Choice Behavior: A Theoretical Analysis.</em> Wiley.</li>
  <li>Additional:
    <ul>
      <li>Ben-Akiva, M.; Lerman, S. (1985). <em>Discrete Choice Analysis: Theory and Application to Travel Demand.</em> Transportation Studies. Massachusetts: MIT Press.</li>
      <li>Park, Byeong U.; Simar, Léopold; Zelenyuk, Valentin (2017). "Nonparametric estimation of dynamic discrete choice models for time series data" (PDF). <em>Computational Statistics & Data Analysis.</em> 108: 97–120. doi:10.1016/j.csda.2016.10.024.</li>
      <li>Rafailov, Rafael, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. "Direct preference optimization: Your language model is secretly a reward model." arXiv preprint arXiv:2305.18290 (2023).</li>
    </ul>
  </li>
</ul>
</small>
</section>

  


</div>
      

    </div>
<script type="text/javascript" src=/class/cs329h/slides/reveal-hugo/object-assign.js></script>


<script src="/class/cs329h/slides/reveal-js/dist/reveal.js"></script>


  <script type="text/javascript" src="/class/cs329h/slides/reveal-js/plugin/markdown/markdown.js"></script>
  
  <script type="text/javascript" src="/class/cs329h/slides/reveal-js/plugin/highlight/highlight.js"></script>
  
  <script type="text/javascript" src="/class/cs329h/slides/reveal-js/plugin/zoom/zoom.js"></script>
  
  <script type="text/javascript" src="/class/cs329h/slides/reveal-js/plugin/notes/notes.js"></script>
  
<script type="text/javascript">
  
  
  function camelize(map) {
    if (map) {
      Object.keys(map).forEach(function(k) {
        newK = k.replace(/(\_\w)/g, function(m) { return m[1].toUpperCase() });
        if (newK != k) {
          map[newK] = map[k];
          delete map[k];
        }
      });
    }
    return map;
  }

  var revealHugoDefaults = { center: true, controls: true, history: true, progress: true, transition: "slide" };
  var revealHugoSiteParams = {"custom_theme":"css/serif.css","enablesourcemap":true,"history":true,"margin":0.1,"slide_number":true};
  var revealHugoPageParams = {};

  var revealHugoPlugins = {
    
    plugins: [RevealMarkdown,RevealHighlight,RevealZoom,RevealNotes]
  };

  
  var options = Object.assign({},
    camelize(revealHugoDefaults),
    camelize(revealHugoSiteParams),
    camelize(revealHugoPageParams),
    camelize(revealHugoPlugins));

  Reveal.initialize(options);
</script>





  
  
    
  

  
  
    
  

  
  

  
  

  
  

  
  





<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
  

<script type="text/javascript" id="MathJax-script" async src="/class/cs329h/slides/tex-svg_7522271970123696654.js"></script>

    
    
  </body>
</html>
